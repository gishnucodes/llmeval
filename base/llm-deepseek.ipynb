{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T00:25:05.910454Z",
     "start_time": "2025-04-14T00:24:57.126640Z"
    }
   },
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForImageTextToText, GenerationConfig, pipeline\n",
    "import torch\n",
    "import torch.nn as nn"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T00:29:02.080020Z",
     "start_time": "2025-04-14T00:28:33.121534Z"
    }
   },
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "    attn_implementation=\"eager\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ").to(device=\"mps\")\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `eager`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T00:29:07.634905Z",
     "start_time": "2025-04-14T00:29:07.585907Z"
    }
   },
   "source": [
    "prompt = \"tommorow is the place for all good good zombies come to the aid\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T00:29:19.735030Z",
     "start_time": "2025-04-14T00:29:09.901847Z"
    }
   },
   "source": [
    "with torch.no_grad():\n",
    "    raw_outputs = model(**inputs)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T00:29:30.635069Z",
     "start_time": "2025-04-14T00:29:30.604272Z"
    }
   },
   "source": [
    "prob = torch.softmax(raw_outputs.logits, dim=-1)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T00:29:38.792223Z",
     "start_time": "2025-04-14T00:29:38.693972Z"
    }
   },
   "source": [
    "top_pr, top_id = torch.topk(prob[0, -1, :], 50)\n",
    "\n",
    "for t_id, t_pr in zip(top_id, top_pr):\n",
    "    t_str = tokenizer.decode(t_id.item()).strip()\n",
    "    print(f\"'{t_str}': {t_pr.item():.4f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'of': 0.5586\n",
      "',': 0.1240\n",
      "'.': 0.0664\n",
      "'!': 0.0356\n",
      "'...': 0.0229\n",
      "'.': 0.0216\n",
      "'': 0.0190\n",
      "'...': 0.0179\n",
      "'in': 0.0139\n",
      "'during': 0.0115\n",
      "'!\"': 0.0074\n",
      "'!': 0.0070\n",
      "'.': 0.0062\n",
      "'!': 0.0058\n",
      "'...': 0.0055\n",
      "'you': 0.0051\n",
      "'when': 0.0051\n",
      "'...\"': 0.0027\n",
      "'': 0.0021\n",
      "'if': 0.0019\n",
      "'and': 0.0017\n",
      "'?': 0.0014\n",
      "'with': 0.0013\n",
      "',': 0.0011\n",
      "'from': 0.0011\n",
      "'\"': 0.0011\n",
      "'me': 0.0011\n",
      "'?': 0.0011\n",
      "'.\"': 0.0011\n",
      "'t': 0.0010\n",
      "'for': 0.0010\n",
      "'on': 0.0010\n",
      "',\"': 0.0010\n",
      "'as': 0.0009\n",
      "'\"': 0.0008\n",
      "'(': 0.0008\n",
      "'.\"': 0.0007\n",
      "'\\n': 0.0007\n",
      "';': 0.0007\n",
      "'\",': 0.0007\n",
      "'…': 0.0006\n",
      "':': 0.0005\n",
      "'is': 0.0005\n",
      "'at': 0.0005\n",
      "'}': 0.0005\n",
      "'through': 0.0005\n",
      "'\"': 0.0004\n",
      "'today': 0.0004\n",
      "'': 0.0004\n",
      "'...': 0.0004\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T00:29:49.164328Z",
     "start_time": "2025-04-14T00:29:49.156881Z"
    }
   },
   "source": [
    "gencfg = GenerationConfig(do_sample=True, temperature=0.7)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T00:29:58.363372Z",
     "start_time": "2025-04-14T00:29:51.835710Z"
    }
   },
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        generation_config=gencfg,\n",
    "        max_new_tokens=5,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True\n",
    "    )"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T00:30:08.309740Z",
     "start_time": "2025-04-14T00:30:08.254623Z"
    }
   },
   "source": [
    "top_sc, top_id = torch.topk(outputs.scores[0]. squeeze(), 50)\n",
    "for token_id, score in zip(top_id, top_sc):\n",
    "    token_str = tokenizer.decode(token_id.item()).strip()\n",
    "    print(f\"'{token_str}': {score.item():.4f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'of': 24.4643\n",
      "',': 22.3214\n",
      "'.': 21.4286\n",
      "'!': 20.5357\n",
      "'...': 19.9107\n",
      "'.': 19.8214\n",
      "'': 19.6429\n",
      "'...': 19.5536\n",
      "'in': 19.1964\n",
      "'during': 18.9286\n",
      "'!\"': 18.3036\n",
      "'!': 18.2143\n",
      "'.': 18.0357\n",
      "'!': 17.9464\n",
      "'...': 17.8571\n",
      "'you': 17.7679\n",
      "'when': 17.7679\n",
      "'...\"': 16.8750\n",
      "'': 16.5179\n",
      "'if': 16.3393\n",
      "'and': 16.1607\n",
      "'?': 15.8929\n",
      "'with': 15.8036\n",
      "',': 15.6250\n",
      "'from': 15.6250\n",
      "'\"': 15.6250\n",
      "'me': 15.6250\n",
      "'?': 15.5357\n",
      "'.\"': 15.5357\n",
      "'t': 15.4464\n",
      "'for': 15.4464\n",
      "'on': 15.4464\n",
      "',\"': 15.4464\n",
      "'as': 15.3571\n",
      "'\"': 15.1786\n",
      "'(': 15.1786\n",
      "'.\"': 15.0000\n",
      "'\\n': 14.9107\n",
      "';': 14.8214\n",
      "'\",': 14.8214\n",
      "'…': 14.6429\n",
      "':': 14.5536\n",
      "'is': 14.5536\n",
      "'at': 14.5536\n",
      "'}': 14.5536\n",
      "'through': 14.5536\n",
      "'\"': 14.2857\n",
      "'today': 14.1964\n",
      "'': 14.0179\n",
      "'...': 14.0179\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "gencfg = GenerationConfig(do_sample=True, temperature=1.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        generation_config=gencfg,\n",
    "        max_new_tokens=5,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'to': 14.5536\n",
      "'in': 13.0357\n",
      "'.': 12.0536\n",
      "',': 11.9643\n",
      "'...': 11.8750\n",
      "'...': 11.6071\n",
      "'.': 11.1607\n",
      "'that': 11.1161\n",
      "'and': 10.7589\n",
      "'happening': 10.4464\n",
      "'are': 10.4464\n",
      "'!': 10.3571\n",
      "'': 10.1786\n",
      "'happen': 10.0446\n",
      "'coming': 10.0000\n",
      "'come': 9.9554\n",
      "'.': 9.9107\n",
      "'is': 9.9107\n",
      "',': 9.8661\n",
      "'...': 9.8214\n",
      "'...': 9.7321\n",
      "'from': 9.6429\n",
      "'!': 9.5536\n",
      "'about': 9.3304\n",
      "'happens': 9.1964\n",
      "'?': 9.1964\n",
      "'...,': 9.1964\n",
      "'...': 9.1071\n",
      "'except': 9.1071\n",
      "'—': 9.1071\n",
      "'comes': 9.0179\n",
      "'for': 9.0179\n",
      "'must': 8.9732\n",
      "'you': 8.9732\n",
      "'should': 8.7500\n",
      "'will': 8.6161\n",
      "'?': 8.6161\n",
      "'minus': 8.5268\n",
      "'(': 8.4821\n",
      "'as': 8.4375\n",
      "'have': 8.3929\n",
      "'/': 8.3929\n",
      "''': 8.3929\n",
      "'now': 8.3482\n",
      "'living': 8.3036\n",
      "'...\\': 8.3036\n",
      "'around': 8.3036\n",
      "';': 8.2143\n",
      "'related': 8.1696\n",
      "'at': 8.1696\n"
     ]
    }
   ],
   "source": [
    "top_sc, top_id = torch.topk(outputs.scores[1].squeeze(), 50)\n",
    "for token_id, score in zip(top_id, top_sc):\n",
    "    token_str = tokenizer.decode(token_id.item()).strip()\n",
    "    print(f\"'{token_str}': {score.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
