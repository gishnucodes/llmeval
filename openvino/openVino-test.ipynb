{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-30T22:35:53.101927Z",
     "start_time": "2025-04-30T21:41:32.540721Z"
    }
   },
   "source": [
    "# Ensure necessary libraries are installed:\n",
    "# pip install optimum[openvino] openvino-dev transformers torch>=2.1.0 accelerate huggingface_hub sentencepiece protobuf<=3.20.3 numpy\n",
    "\n",
    "import numpy as np\n",
    "import openvino as ov  # Use openvino directly for Core if needed, but Optimum abstracts much of this\n",
    "from optimum.intel import OVModelForCausalLM  # <<< Use Optimum Intel for HF integration\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "import gc\n",
    "import time\n",
    "import os\n",
    "\n",
    "# from huggingface_hub import login # Keep if needed for gated models during conversion/tokenizer download\n",
    "# import os\n",
    "\n",
    "# # Login logic (only needed once usually for conversion/download, not runtime if files are local)\n",
    "# hf_token = os.environ.get(\"HUGGING_FACE_HUB_TOKEN\")\n",
    "# if hf_token:\n",
    "#     print(\"Logging in to Hugging Face Hub (may be needed for tokenizer download)...\")\n",
    "#     try:\n",
    "#         login(token=hf_token)\n",
    "#     except Exception as e:\n",
    "#         print(f\"HF Login failed (might be okay if tokenizer is cached): {e}\")\n",
    "# else:\n",
    "#     print(\"HF Token not found. Gated model download/tokenizer fetch might fail if not cached.\")\n",
    "\n",
    "# <<< Path to the *converted* OpenVINO model directory >>>\n",
    "OPENVINO_MODEL_DIR = \"openvino_model/\"  # <<< Directory created by optimum-cli export\n",
    "# <<< Get the original model name for metadata, but we load from the OV dir >>>\n",
    "# It's good practice to store the original name with the converted model,\n",
    "# optimum usually saves a config.json that might retain this.\n",
    "try:\n",
    "    config = AutoConfig.from_pretrained(OPENVINO_MODEL_DIR)\n",
    "    ORIGINAL_MODEL_NAME = getattr(config, \"_name_or_path\", \"Unknown (Check openvino_model/config.json)\")\n",
    "    print(f\"Inferred original model name: {ORIGINAL_MODEL_NAME}\")\n",
    "except Exception:\n",
    "    ORIGINAL_MODEL_NAME = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"  # Fallback\n",
    "    print(f\"Could not infer original model name, using fallback: {ORIGINAL_MODEL_NAME}\")\n",
    "\n",
    "# <<< OpenVINO device selection >>>\n",
    "# Options: \"CPU\", \"GPU\", \"GPU.0\", \"GPU.1\", \"NPU\", \"AUTO\"\n",
    "# \"AUTO\" tries to pick the best available device.\n",
    "DEVICE = \"AUTO\"\n",
    "print(f\"Attempting to use OpenVINO device: {DEVICE}\")\n",
    "# Note: OpenVINO handles precision (FP32, FP16, INT8) based on the converted model\n",
    "# or via compilation settings. We don't set torch_dtype here.\n",
    "\n",
    "DEPTH_RANGE = 1\n",
    "# Ensure INPUT_FILE path is correct for your environment\n",
    "INPUT_FILE = 'feed.txt'  # Assuming it's in the same directory or provide full path\n",
    "# Create the input file if it doesn't exist for testing\n",
    "if not os.path.exists(INPUT_FILE):\n",
    "    print(f\"Warning: Input file '{INPUT_FILE}' not found. Creating a dummy file.\")\n",
    "    with open(INPUT_FILE, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"The quick brown fox jumps over the lazy dog\")\n",
    "\n",
    "OUTPUT_FILE = f\"feed-kv-cache-openvino.lif\"  # Changed output filename\n",
    "\n",
    "# <<< Context window from model config (usually loaded automatically by Optimum) >>>\n",
    "# MODEL_CONTEXT_WINDOW = 128_000 # Let Optimum/config handle this\n",
    "\n",
    "print(\"Step 1 & 2: Loading OpenVINO model and tokenizer...\")\n",
    "\n",
    "try:\n",
    "    # <<< Load the tokenizer from the directory containing the OV model >>>\n",
    "    # Optimum saves the tokenizer files alongside the .xml/.bin during export\n",
    "    tokenizer = AutoTokenizer.from_pretrained(OPENVINO_MODEL_DIR, trust_remote_code=True)\n",
    "    print(\"  Tokenizer loaded successfully from OpenVINO model directory.\")\n",
    "\n",
    "    # <<< Load the OpenVINO model using Optimum Intel >>>\n",
    "    # This handles compiling the model for the target device and setting up KV cache\n",
    "    model = OVModelForCausalLM.from_pretrained(\n",
    "        OPENVINO_MODEL_DIR,\n",
    "        device=DEVICE,\n",
    "        trust_remote_code=True,\n",
    "        # ov_config={\"PERFORMANCE_HINT\": \"LATENCY\"} # Optional: Optimize for latency or throughput\n",
    "    )\n",
    "    print(f\"  OpenVINO Model loaded and compiled for device {model.device}.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to load OpenVINO model or tokenizer from {OPENVINO_MODEL_DIR}\")\n",
    "    print(f\"Ensure the directory exists and contains openvino_model.xml/bin and tokenizer files.\")\n",
    "    print(f\"Error details: {e}\")\n",
    "    exit()\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    print(\"  Tokenizer missing pad token; setting pad_token = eos_token\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    # Update model config if possible (Optimum might handle this internally)\n",
    "    if hasattr(model, 'config'):\n",
    "        model.config.pad_token_id = tokenizer.eos_token_id\n",
    "print(\"  Model and Tokenizer ready for inference.\\n\")\n",
    "\n",
    "# <<< No PyTorch Quantization needed - OV uses its own (often applied during conversion) >>>\n",
    "# print(\"Step 2: Applying dynamic quantization for faster CPU inference...\") >>> Removed\n",
    "\n",
    "print(\"Step 4: Prompting user for inputs...\")\n",
    "promptID = input(\"  Enter Prompt ID [Default: VanityTestKVCacheOV]: \") or \"VanityTestKVCacheOV\"  # <<< Changed default\n",
    "MultiPV_str = input(\"  Enter MultiPV (top logits to show) [Default: 5]: \") or \"5\"\n",
    "MultiPV = int(MultiPV_str)\n",
    "LegalNumberOfMove_str = input(\"  Enter Max Number of moves [Default: 10]: \") or \"10\"\n",
    "LegalNumberOfMove = int(LegalNumberOfMove_str)\n",
    "EngineID = f\"{ORIGINAL_MODEL_NAME} OpenVINO ({model.device})\"  # <<< Updated EngineID\n",
    "Depth = 1\n",
    "print(\"  User inputs captured.\\n\")\n",
    "\n",
    "print(\"Step 5: Pre-processing input sequence...\")\n",
    "initial_prompt = \"Complete successive parts of a sentence given one word at a time:\"\n",
    "initial_prompt_ids = tokenizer.encode(initial_prompt, add_special_tokens=False)\n",
    "\n",
    "print(f\"  Reading words from {INPUT_FILE}...\")\n",
    "try:\n",
    "    with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n",
    "        words_from_file = f.read().split()\n",
    "    print(f\"  Found {len(words_from_file)} words.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Input file '{INPUT_FILE}' not found. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "# Store tokenized words separately for easier access in the loop\n",
    "tokenized_words = []\n",
    "total_token_count = len(initial_prompt_ids)\n",
    "processed_words = []  # Store the actual words processed to match tokenized_words\n",
    "\n",
    "print(\"  Tokenizing words...\")\n",
    "for word in words_from_file:\n",
    "    # Add space prefix expected by many tokenizers for words after the first\n",
    "    word_tokens = tokenizer.encode(\" \" + word, add_special_tokens=False)\n",
    "    if not word_tokens:  # Handle cases where a word might tokenize to nothing\n",
    "        print(f\"  Warning: Word '{word}' tokenized to empty sequence, skipping.\")\n",
    "        continue\n",
    "    tokenized_words.append(word_tokens)\n",
    "    processed_words.append(word)  # Keep track of words we actually tokenized\n",
    "    total_token_count += len(word_tokens)\n",
    "\n",
    "print(f\"  Pre-processed {len(processed_words)} words.\")\n",
    "print(f\"  Total estimated tokens (prompt + words): {total_token_count}\\n\")\n",
    "\n",
    "# <<< Context window check - Optimum model config should have this >>>\n",
    "model_context_window = getattr(model.config, \"max_position_embeddings\", None)\n",
    "if model_context_window:\n",
    "    print(f\"  Model context window: {model_context_window}\")\n",
    "    if total_token_count > model_context_window:\n",
    "        print(\n",
    "            f\"WARNING: Estimated total token count ({total_token_count}) exceeds model context window ({model_context_window}).\")\n",
    "        print(\"KV caching might fail or produce unexpected results if the limit is strictly enforced by the model.\")\n",
    "else:\n",
    "    print(\"Warning: Could not determine model context window from config.\")\n",
    "\n",
    "num_words_to_process = min(len(processed_words), LegalNumberOfMove)\n",
    "if num_words_to_process < len(processed_words):\n",
    "    print(f\"  Will process the first {num_words_to_process} words due to LegalNumberOfMove limit.\\n\")\n",
    "elif num_words_to_process == 0:\n",
    "    print(\"  Warning: No words to process based on input file or limits.\\n\")\n",
    "\n",
    "print(\"Step 8: Preparing output file header...\")  # Step numbers kept for consistency\n",
    "header_lines = [\n",
    "    f'[PromptID \"{promptID}\"]\\n',\n",
    "    f'[EngineID \"{EngineID}\"]\\n',\n",
    "    f'[MultiPV \"{MultiPV}\"]\\n',\n",
    "    f'[DepthRange \"1:1\"]\\n\\n',\n",
    "    \"1-0\\n\"\n",
    "]\n",
    "print(\"  Header prepared.\\n\")\n",
    "\n",
    "print(\"Step 9: Entering main generation loop (using OpenVINO KV Caching)...\\n\")\n",
    "PrevEval = \"n.a.\"\n",
    "start_time = time.time()\n",
    "# <<< KV Caching Change: Optimum's OVModelForCausalLM handles the past_key_values internally >>>\n",
    "# We need to pass `use_cache=True` and it returns the cache object\n",
    "past_key_values = None\n",
    "\n",
    "if num_words_to_process > 0:\n",
    "    with open(OUTPUT_FILE, 'w', encoding='utf-8') as writer:\n",
    "        print(\"  Writing header to output file...\")\n",
    "        writer.write(''.join(header_lines))\n",
    "        print(\"  Header written. Starting word-by-word prediction.\\n\")\n",
    "\n",
    "        # <<< Process the initial prompt first to build the initial cache >>>\n",
    "        print(\"  9.0: Processing initial prompt to build cache...\")\n",
    "        # <<< Input needs to be NumPy array or PyTorch tensor >>>\n",
    "        initial_input_ids = np.array([initial_prompt_ids], dtype=np.int64)\n",
    "        start_time_gen = time.time()\n",
    "\n",
    "        # Use model's __call__ or generate method (call is lower level, similar to torch)\n",
    "        # No need for torch.no_grad() with OpenVINO\n",
    "        outputs = model(\n",
    "            input_ids=initial_input_ids,\n",
    "            past_key_values=None,  # No cache yet\n",
    "            use_cache=True,  # Request cache output\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False,\n",
    "        )\n",
    "\n",
    "        gen_duration = time.time() - start_time_gen\n",
    "        # <<< Optimum returns cache in outputs.past_key_values >>>\n",
    "        past_key_values = outputs.past_key_values\n",
    "        print(f\"      Initial prompt processing took: {gen_duration:.4f} seconds\")\n",
    "        # Logits for prediction *after* the prompt\n",
    "        # logits_after_prompt = outputs.logits[:, -1, :] # Shape: [batch_size, vocab_size]\n",
    "\n",
    "        # Now loop through the words, using the cache\n",
    "        for turnCount in range(1, num_words_to_process + 1):\n",
    "            current_word = processed_words[turnCount - 1]\n",
    "            current_word_tokens = tokenized_words[turnCount - 1]\n",
    "            print(f\"Turn {turnCount}: Processing word '{current_word}' ({len(current_word_tokens)} tokens)\")\n",
    "\n",
    "            # <<< Input is just the current word's tokens (as NumPy) >>>\n",
    "            input_ids = np.array([current_word_tokens], dtype=np.int64)\n",
    "            current_input_len = input_ids.shape[1]\n",
    "\n",
    "            start_time_gen = time.time()\n",
    "            print(f\"  9.4: Running OpenVINO model.forward() with {current_input_len} new tokens (using KV cache)...\")\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                past_key_values=past_key_values,  # <<< Pass the cache received from previous step\n",
    "                use_cache=True,  # <<< Request updated cache\n",
    "                output_attentions=False,\n",
    "                output_hidden_states=False,\n",
    "            )\n",
    "            end_time_gen = time.time()\n",
    "            gen_duration = end_time_gen - start_time_gen\n",
    "            print(f\"      Model forward pass took: {gen_duration:.4f} seconds\")\n",
    "\n",
    "            # <<< Update the cache for the next iteration >>>\n",
    "            past_key_values = outputs.past_key_values\n",
    "\n",
    "            # <<< Get logits for the *next* token prediction >>>\n",
    "            # Logits are returned as torch tensors by default in Optimum, even from OV\n",
    "            # Access the raw logits tensor\n",
    "            logits_for_step = outputs.logits[:, -1, :]  # Shape: [batch_size, vocab_size]\n",
    "            logits_for_prediction = logits_for_step[0]  # Shape: [vocab_size]\n",
    "\n",
    "            # <<< Use torch.topk on the logits tensor >>>\n",
    "            # Need torch if Optimum returns torch tensors\n",
    "            # import torch # Make sure torch is imported if not already\n",
    "            try:\n",
    "                import torch\n",
    "            except ImportError:\n",
    "                print(\"Error: PyTorch is required for logit processing even with OpenVINO via Optimum.\")\n",
    "                exit()\n",
    "\n",
    "            # Move logits to CPU if they somehow ended up elsewhere (unlikely with OV CPU/AUTO)\n",
    "            logits_for_prediction_cpu = logits_for_prediction.cpu()\n",
    "\n",
    "            top_k_logits_values, top_k_logits_indices = torch.topk(\n",
    "                logits_for_prediction_cpu, k=MultiPV, dim=-1\n",
    "            )\n",
    "\n",
    "            # Convert results to lists for processing\n",
    "            top_k_logits_values = top_k_logits_values.tolist()\n",
    "            top_k_logits_indices = top_k_logits_indices.tolist()\n",
    "\n",
    "            # Decode the top K tokens based on logits\n",
    "            top_k_tokens = [tokenizer.decode(tid) for tid in top_k_logits_indices]\n",
    "\n",
    "            print(f\"      Top {MultiPV} Logits (Token | Logit Value):\")\n",
    "            for i in range(MultiPV):\n",
    "                token_str_cleaned = top_k_tokens[i].strip()\n",
    "                print(f\"        - '{token_str_cleaned}': {top_k_logits_values[i]:.4f} (ID: {top_k_logits_indices[i]})\")\n",
    "\n",
    "            # 9.5 Derive primary metrics USING THE TOP LOGITS\n",
    "            modelToken = top_k_tokens[0].strip()  # Token with the highest logit\n",
    "            modelEval = f\"{top_k_logits_values[0]:.4f}\"\n",
    "            modelEval = round(float(modelEval) * 100)  # Original scaling\n",
    "            NextEval = (f\"{top_k_logits_values[1]:.4f}\" if MultiPV > 1 else \"n.a.\")\n",
    "            NextEval = round(float(NextEval) * 100) if MultiPV > 1 and isinstance(top_k_logits_values[1],\n",
    "                                                                                  float) else \"n.a.\"  # Original scaling\n",
    "            print(f\"  9.5: Top token prediction: '{modelToken}' (Eval: {modelEval}) | Next best Eval: {NextEval}\")\n",
    "\n",
    "            # 9.6 Build lines for this turn\n",
    "            print(\"  9.6: Building output lines for this turn...\")\n",
    "            current_stem = initial_prompt + \" \" + \" \".join(processed_words[:turnCount])\n",
    "\n",
    "            lines = [\n",
    "                f'[PID \"{promptID}\"]\\n',\n",
    "                f'[EID \"{ORIGINAL_MODEL_NAME}\"]\\n',  # Use original name for consistency\n",
    "                f'[Turn \"{turnCount}-w\"]\\n',\n",
    "                f'[TextToken \"{current_word}:\"]\\n',\n",
    "                f'[ModelToken \"{modelToken}:\"]\\n',  # The model's top prediction\n",
    "                f'[Eval \"{modelEval}\"]\\n',\n",
    "                f'[PrevEval \"{PrevEval}\"]\\n',\n",
    "                f'[NextEval \"{NextEval}\"]\\n',\n",
    "                f'[Depth \"{Depth}\"]\\n',\n",
    "                f'[STEM \"{current_stem}\"]\\n',  # Stem includes the word just processed\n",
    "                f'[NumLegalMoves \"{LegalNumberOfMove}\"]\\n',\n",
    "                \"---------------\\n\",\n",
    "                f\"{DEPTH_RANGE}\",  # Should likely be Depth? Kept as is from original.\n",
    "                \"---------------\\n\"\n",
    "            ]\n",
    "            # Append the list of top K tokens and their raw logits\n",
    "            for token_str, logit_val in zip(top_k_tokens, top_k_logits_values):\n",
    "                lines.append(f\"{token_str.strip()}: {logit_val:.4f}\\n\")\n",
    "\n",
    "            lines.append(\n",
    "                \"===========================================================================================================\\n\\n\")\n",
    "            lines.append(f\"[Comments]\\n\")\n",
    "            lines.append(f\"[EndMove]\\n\\n\")\n",
    "            print(\"      Lines built.\")\n",
    "\n",
    "            # 9.7 Write to file\n",
    "            print(\"  9.7: Writing lines to output file...\")\n",
    "            writer.write(''.join(lines))\n",
    "            print(\"      Write complete.\\n\")\n",
    "\n",
    "            # 9.8 Update state\n",
    "            PrevEval = modelEval\n",
    "\n",
    "            # 9.9 Status update\n",
    "            status_interval = min(100, num_words_to_process // 2 if num_words_to_process >= 10 else 10)\n",
    "            if turnCount % status_interval == 0 or turnCount == num_words_to_process:\n",
    "                elapsed = time.time() - start_time\n",
    "                rate = turnCount / elapsed if elapsed > 0 else 0\n",
    "                # Report tokens/sec as well\n",
    "                current_total_tokens = len(initial_prompt_ids) + sum(len(tk) for tk in tokenized_words[:turnCount])\n",
    "                token_rate = current_total_tokens / elapsed if elapsed > 0 else 0\n",
    "                print(\n",
    "                    f\"  Status: Processed {turnCount}/{num_words_to_process} words at {rate:.2f} w/s ({token_rate:.2f} tok/s) ({elapsed:.2f}s total)\\n\")\n",
    "\n",
    "        print(\"  Finished processing requested words.\\n\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping main generation loop as there are no words to process.\")\n",
    "\n",
    "print(\"Step 10: Reporting final statistics...\")\n",
    "total_time = time.time() - start_time\n",
    "avg_rate_words = (num_words_to_process / total_time) if total_time > 0 and num_words_to_process > 0 else 0\n",
    "# Calculate final token rate\n",
    "final_total_tokens = len(initial_prompt_ids) + sum(len(tk) for tk in tokenized_words[:num_words_to_process])\n",
    "avg_rate_tokens = (final_total_tokens / total_time) if total_time > 0 and final_total_tokens > 0 else 0\n",
    "\n",
    "print(f\"  Total turns processed: {num_words_to_process}\")\n",
    "print(f\"  Total tokens processed (including prompt): {final_total_tokens}\")\n",
    "print(f\"  Total time: {total_time:.2f} seconds\")\n",
    "print(f\"  Average speed: {avg_rate_words:.2f} words/second\")\n",
    "print(f\"  Average speed: {avg_rate_tokens:.2f} tokens/second\")  # Report token speed\n",
    "print(f\"  Output written to {OUTPUT_FILE}\")\n",
    "\n",
    "# Optional: Clean up memory\n",
    "print(\"\\nCleaning up resources...\")\n",
    "del model\n",
    "del tokenizer\n",
    "del tokenized_words\n",
    "if 'initial_input_ids' in locals():\n",
    "    del initial_input_ids\n",
    "if 'input_ids' in locals():\n",
    "    del input_ids\n",
    "if 'outputs' in locals():\n",
    "    del outputs\n",
    "if 'past_key_values' in locals():\n",
    "    del past_key_values  # Clear cache\n",
    "# OpenVINO objects (like compiled model) are managed by the OVModelForCausalLM object\n",
    "# No explicit CUDA cache to clear\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\nScript finished.\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\conda\\envs\\py_310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\conda\\envs\\py_310\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Inferred original model name: openvino_model/\n",
      "Attempting to use OpenVINO device: AUTO\n",
      "Step 1 & 2: Loading OpenVINO model and tokenizer...\n",
      "  Tokenizer loaded successfully from OpenVINO model directory.\n",
      "  OpenVINO Model loaded and compiled for device cpu.\n",
      "  Model and Tokenizer ready for inference.\n",
      "\n",
      "Step 4: Prompting user for inputs...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 97\u001B[0m\n\u001B[0;32m     93\u001B[0m \u001B[38;5;66;03m# <<< No PyTorch Quantization needed - OV uses its own (often applied during conversion) >>>\u001B[39;00m\n\u001B[0;32m     94\u001B[0m \u001B[38;5;66;03m# print(\"Step 2: Applying dynamic quantization for faster CPU inference...\") >>> Removed\u001B[39;00m\n\u001B[0;32m     96\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mStep 4: Prompting user for inputs...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 97\u001B[0m promptID \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43minput\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m  Enter Prompt ID [Default: VanityTestKVCacheOV]: \u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mVanityTestKVCacheOV\u001B[39m\u001B[38;5;124m\"\u001B[39m  \u001B[38;5;66;03m# <<< Changed default\u001B[39;00m\n\u001B[0;32m     98\u001B[0m MultiPV_str \u001B[38;5;241m=\u001B[39m \u001B[38;5;28minput\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m  Enter MultiPV (top logits to show) [Default: 5]: \u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m5\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     99\u001B[0m MultiPV \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m(MultiPV_str)\n",
      "File \u001B[1;32mC:\\conda\\envs\\py_310\\lib\\site-packages\\ipykernel\\kernelbase.py:1282\u001B[0m, in \u001B[0;36mKernel.raw_input\u001B[1;34m(self, prompt)\u001B[0m\n\u001B[0;32m   1280\u001B[0m     msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mraw_input was called, but this frontend does not support input requests.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1281\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m StdinNotImplementedError(msg)\n\u001B[1;32m-> 1282\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_input_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1283\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1284\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_parent_ident\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mshell\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1285\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_parent\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mshell\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1286\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpassword\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m   1287\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\conda\\envs\\py_310\\lib\\site-packages\\ipykernel\\kernelbase.py:1325\u001B[0m, in \u001B[0;36mKernel._input_request\u001B[1;34m(self, prompt, ident, parent, password)\u001B[0m\n\u001B[0;32m   1322\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m:\n\u001B[0;32m   1323\u001B[0m     \u001B[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001B[39;00m\n\u001B[0;32m   1324\u001B[0m     msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInterrupted by user\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m-> 1325\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m(msg) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1326\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[0;32m   1327\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlog\u001B[38;5;241m.\u001B[39mwarning(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid Message:\u001B[39m\u001B[38;5;124m\"\u001B[39m, exc_info\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: Interrupted by user"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
